model_name: 'transformer'
lr_init: 0.001
start_decay: 0
end_decay: ~
end_warm: ~
lr_warm: 0.0001
lr_min: 0.0001
batch_size: 128