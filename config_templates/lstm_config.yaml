img2seq:
  img_w: 512
  img_h: 256
  dim_encoder: 512
  dim_decoder: 256
  dim_attention: 512
  dim_embed:
  dropout: 0.1
model_name: 'lstm'
max_seq_len: 300
lr_init: 0.001
lr_min: 0.0001
batch_size: 1
eval_batch_size: 128
lr_method: 'adam'
lr_scheduler: 'CosineAnnealingLR'
device: 'cuda:0'
criterion_method: 'CrossEntropyLoss'
n_epochs: 10
SCST_predict_mode: 'greedy'
SCST_lr: 0.00001
gradient_accumulate_num: 8
beam_width: 5